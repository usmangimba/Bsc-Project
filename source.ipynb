{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyFTS import *\n",
    "from pyFTS.common import Membership\n",
    "\n",
    "\n",
    "class FuzzySet(object):\n",
    "    \"\"\"\n",
    "    Fuzzy Set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, mf, parameters, centroid, alpha=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a Fuzzy Set\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        \"\"\"The fuzzy set name\"\"\"\n",
    "        self.mf = mf\n",
    "        \"\"\"The membership function\"\"\"\n",
    "        self.parameters = parameters\n",
    "        \"\"\"The parameters of the membership function\"\"\"\n",
    "        self.centroid = centroid\n",
    "        \"\"\"The fuzzy set center of mass (or midpoint)\"\"\"\n",
    "        self.alpha = alpha\n",
    "        \"\"\"The alpha cut value\"\"\"\n",
    "        self.type = kwargs.get('type', 'common')\n",
    "        \"\"\"The fuzzy set type (common, composite, nonstationary, etc)\"\"\"\n",
    "        self.variable = kwargs.get('variable', None)\n",
    "        \"\"\"In multivariate time series, indicate for which variable this fuzzy set belogs\"\"\"\n",
    "        self.Z = None\n",
    "        \"\"\"Partition function in respect to the membership function\"\"\"\n",
    "\n",
    "        if parameters is not None:\n",
    "            if self.mf == Membership.gaussmf:\n",
    "                self.lower = parameters[0] - parameters[1] * 3\n",
    "                self.upper = parameters[0] + parameters[1] * 3\n",
    "            elif self.mf == Membership.sigmf:\n",
    "                k = (parameters[1] / (2 * parameters[0]))\n",
    "                self.lower = parameters[1] - k\n",
    "                self.upper = parameters[1] + k\n",
    "            else:\n",
    "                self.lower = min(parameters)\n",
    "                self.upper = max(parameters)\n",
    "\n",
    "        self.metadata = {}\n",
    "\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"\n",
    "        Preprocess the data point for non native types\n",
    "\n",
    "        :param x:\n",
    "        :return: return a native type value for the structured type\n",
    "        \"\"\"\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def membership(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the membership value of a given input\n",
    "\n",
    "        :param x: input value \n",
    "        :return: membership value of x at this fuzzy set\n",
    "        \"\"\"\n",
    "        return self.mf(self.transform(x), self.parameters) * self.alpha\n",
    "\n",
    "\n",
    "\n",
    "    def partition_function(self, uod=None, nbins=100):\n",
    "        \"\"\"\n",
    "        Calculate the partition function over the membership function.\n",
    "\n",
    "        :param uod:\n",
    "        :param nbins:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.Z is None and uod is not None:\n",
    "            self.Z = 0.0\n",
    "            for k in np.linspace(uod[0], uod[1], nbins):\n",
    "                self.Z += self.membership(k)\n",
    "\n",
    "        return self.Z\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name + \": \" + str(self.mf.__name__) + \"(\" + str(self.parameters) + \")\"\n",
    "\n",
    "\n",
    "\n",
    "def __binary_search(x, fuzzy_sets, ordered_sets):\n",
    "    \"\"\"\n",
    "    Search for elegible fuzzy sets to fuzzyfy x\n",
    "\n",
    "    :param x: input value to be fuzzyfied\n",
    "    :param fuzzy_sets:  a dictionary where the key is the fuzzy set name and the value is the fuzzy set object.\n",
    "    :param ordered_sets: a list with the fuzzy sets names ordered by their centroids.\n",
    "    :return: A list with the best fuzzy sets that may contain x\n",
    "    \"\"\"\n",
    "    max_len = len(fuzzy_sets) - 1\n",
    "    first = 0\n",
    "    last = max_len\n",
    "\n",
    "    while first <= last:\n",
    "        midpoint = (first + last) // 2\n",
    "\n",
    "        fs = ordered_sets[midpoint]\n",
    "        fs1 = ordered_sets[midpoint - 1] if midpoint > 0 else ordered_sets[0]\n",
    "        fs2 = ordered_sets[midpoint + 1] if midpoint < max_len else ordered_sets[max_len]\n",
    "\n",
    "        if fuzzy_sets[fs1].centroid <= fuzzy_sets[fs].transform(x) <= fuzzy_sets[fs2].centroid:\n",
    "            return (midpoint - 1, midpoint, midpoint + 1)\n",
    "        elif midpoint <= 1:\n",
    "            return [0]\n",
    "        elif midpoint >= max_len:\n",
    "            return [max_len]\n",
    "        else:\n",
    "            if fuzzy_sets[fs].transform(x) < fuzzy_sets[fs].centroid:\n",
    "                last = midpoint - 1\n",
    "            else:\n",
    "                first = midpoint + 1\n",
    "\n",
    "\n",
    "\n",
    "def fuzzyfy(data, partitioner, **kwargs):\n",
    "    \"\"\"\n",
    "    A general method for fuzzyfication.\n",
    "\n",
    "    :param data: input value to be fuzzyfied\n",
    "    :param partitioner: a trained pyFTS.partitioners.Partitioner object\n",
    "    :param kwargs: dict, optional arguments\n",
    "    :keyword alpha_cut: the minimal membership value to be considered on fuzzyfication (only for mode='sets')\n",
    "    :keyword method: the fuzzyfication method (fuzzy: all fuzzy memberships, maximum: only the maximum membership)\n",
    "    :keyword mode: the fuzzyfication mode (sets: return the fuzzy sets names, vector: return a vector with the membership\n",
    "    values for all fuzzy sets, both: return a list with tuples (fuzzy set, membership value) )\n",
    "    :returns a list with the fuzzyfied values, depending on the mode\n",
    "\n",
    "    \"\"\"\n",
    "    alpha_cut = kwargs.get('alpha_cut', 0.)\n",
    "    mode = kwargs.get('mode', 'sets')\n",
    "    method = kwargs.get('method', 'fuzzy')\n",
    "    if isinstance(data, (list, np.ndarray)):\n",
    "        if mode == 'vector':\n",
    "            return fuzzyfy_instances(data, partitioner.sets, partitioner.ordered_sets)\n",
    "        elif mode == 'both':\n",
    "            mvs = fuzzyfy_instances(data, partitioner.sets, partitioner.ordered_sets)\n",
    "            fs = []\n",
    "            for mv in mvs:\n",
    "                fsets = [(partitioner.ordered_sets[ix], mv[ix])\n",
    "                         for ix in np.arange(len(mv))\n",
    "                         if mv[ix] >= alpha_cut]\n",
    "                fs.append(fsets)\n",
    "            return fs\n",
    "        else:\n",
    "            return fuzzyfy_series(data, partitioner.sets, method, alpha_cut, partitioner.ordered_sets)\n",
    "    else:\n",
    "        if mode == 'vector':\n",
    "            return fuzzyfy_instance(data, partitioner.sets, partitioner.ordered_sets)\n",
    "        elif mode == 'both':\n",
    "            mv = fuzzyfy_instance(data, partitioner.sets, partitioner.ordered_sets)\n",
    "            fsets = [(partitioner.ordered_sets[ix], mv[ix])\n",
    "                     for ix in np.arange(len(mv))\n",
    "                     if mv[ix] >= alpha_cut]\n",
    "            return fsets\n",
    "        else:\n",
    "            return get_fuzzysets(data, partitioner.sets, partitioner.ordered_sets, alpha_cut)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_ordered(fuzzy_sets):\n",
    "    \"\"\"\n",
    "    Order a fuzzy set list by their centroids\n",
    "\n",
    "    :param fuzzy_sets: a dictionary where the key is the fuzzy set name and the value is the fuzzy set object.\n",
    "    :return: a list with the fuzzy sets names ordered by their centroids.\n",
    "    \"\"\"\n",
    "    if len(fuzzy_sets) > 0:\n",
    "        tmp1 = [fuzzy_sets[k] for k in fuzzy_sets.keys()]\n",
    "        return [k.name for k in sorted(tmp1, key=lambda x: x.centroid)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fuzzyfy_instance(inst, fuzzy_sets, ordered_sets=None):\n",
    "    \"\"\"\n",
    "    Calculate the membership values for a data point given fuzzy sets\n",
    "\n",
    "    :param inst: data point\n",
    "    :param fuzzy_sets: a dictionary where the key is the fuzzy set name and the value is the fuzzy set object.\n",
    "    :param ordered_sets: a list with the fuzzy sets names ordered by their centroids.\n",
    "    :return: array of membership values\n",
    "    \"\"\"\n",
    "\n",
    "    if ordered_sets is None:\n",
    "        ordered_sets = set_ordered(fuzzy_sets)\n",
    "\n",
    "    mv = np.zeros(len(fuzzy_sets))\n",
    "\n",
    "    for ix in __binary_search(inst, fuzzy_sets, ordered_sets):\n",
    "        mv[ix] = fuzzy_sets[ordered_sets[ix]].membership(inst)\n",
    "\n",
    "    return mv\n",
    "\n",
    "\n",
    "\n",
    "def fuzzyfy_instances(data, fuzzy_sets, ordered_sets=None):\n",
    "    \"\"\"\n",
    "    Calculate the membership values for a data point given fuzzy sets\n",
    "\n",
    "    :param inst: data point\n",
    "    :param fuzzy_sets: a dictionary where the key is the fuzzy set name and the value is the fuzzy set object.\n",
    "    :param ordered_sets: a list with the fuzzy sets names ordered by their centroids.\n",
    "    :return: array of membership values\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    if ordered_sets is None:\n",
    "        ordered_sets = set_ordered(fuzzy_sets)\n",
    "    for inst in data:\n",
    "        mv = fuzzyfy_instance(inst, fuzzy_sets, ordered_sets)\n",
    "        ret.append(mv)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def get_fuzzysets(inst, fuzzy_sets, ordered_sets=None, alpha_cut=0.0):\n",
    "    \"\"\"\n",
    "    Return the fuzzy sets which membership value for a inst is greater than the alpha_cut\n",
    "\n",
    "    :param inst: data point\n",
    "    :param fuzzy_sets:  a dictionary where the key is the fuzzy set name and the value is the fuzzy set object.\n",
    "    :param ordered_sets: a list with the fuzzy sets names ordered by their centroids.\n",
    "    :param alpha_cut: Minimal membership to be considered on fuzzyfication process\n",
    "    :return: array of membership values\n",
    "    \"\"\"\n",
    "\n",
    "    if ordered_sets is None:\n",
    "        ordered_sets = set_ordered(fuzzy_sets)\n",
    "\n",
    "    try:\n",
    "        fs = [ordered_sets[ix]\n",
    "              for ix in __binary_search(inst, fuzzy_sets, ordered_sets)\n",
    "              if fuzzy_sets[ordered_sets[ix]].membership(inst) > alpha_cut]\n",
    "        return fs\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "\n",
    "\n",
    "\n",
    "def get_maximum_membership_fuzzyset(inst, fuzzy_sets, ordered_sets=None):\n",
    "    \"\"\"\n",
    "    Fuzzify a data point, returning the fuzzy set with maximum membership value\n",
    "\n",
    "    :param inst: data point\n",
    "    :param fuzzy_sets:  a dictionary where the key is the fuzzy set name and the value is the fuzzy set object.\n",
    "    :param ordered_sets: a list with the fuzzy sets names ordered by their centroids.\n",
    "    :return: fuzzy set with maximum membership\n",
    "    \"\"\"\n",
    "    if ordered_sets is None:\n",
    "        ordered_sets = set_ordered(fuzzy_sets)\n",
    "    mv = np.array([fuzzy_sets[key].membership(inst) for key in ordered_sets])\n",
    "    key = ordered_sets[np.argwhere(mv == max(mv))[0, 0]]\n",
    "    return fuzzy_sets[key]\n",
    "\n",
    "\n",
    "\n",
    "def get_maximum_membership_fuzzyset_index(inst, fuzzy_sets):\n",
    "    \"\"\"\n",
    "    Fuzzify a data point, returning the fuzzy set with maximum membership value\n",
    "\n",
    "    :param inst: data point\n",
    "    :param fuzzy_sets: dict of fuzzy sets\n",
    "    :return: fuzzy set with maximum membership\n",
    "    \"\"\"\n",
    "    mv = fuzzyfy_instance(inst, fuzzy_sets)\n",
    "    return np.argwhere(mv == max(mv))[0, 0]\n",
    "\n",
    "\n",
    "\n",
    "def fuzzyfy_series_old(data, fuzzy_sets, method='maximum'):\n",
    "    fts = []\n",
    "    for item in data:\n",
    "        fts.append(get_maximum_membership_fuzzyset(item, fuzzy_sets).name)\n",
    "    return fts\n",
    "\n",
    "\n",
    "\n",
    "def fuzzyfy_series(data, fuzzy_sets, method='maximum', alpha_cut=0.0, ordered_sets=None):\n",
    "    fts = []\n",
    "    if ordered_sets is None:\n",
    "        ordered_sets = set_ordered(fuzzy_sets)\n",
    "    for t, i in enumerate(data):\n",
    "        mv = fuzzyfy_instance(i, fuzzy_sets, ordered_sets)\n",
    "        if len(mv) == 0:\n",
    "            sets = check_bounds(i, fuzzy_sets.items(), ordered_sets)\n",
    "        else:\n",
    "            if method == 'fuzzy':\n",
    "                ix = np.ravel(np.argwhere(mv > alpha_cut))\n",
    "                sets = [fuzzy_sets[ordered_sets[i]].name for i in ix]\n",
    "            elif method == 'maximum':\n",
    "                mx = max(mv)\n",
    "                ix = np.ravel(np.argwhere(mv == mx))\n",
    "                sets = fuzzy_sets[ordered_sets[ix[0]]].name\n",
    "        fts.append(sets)\n",
    "    return fts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grant_bounds(data, fuzzy_sets, ordered_sets):\n",
    "    if data < fuzzy_sets[ordered_sets[0]].lower:\n",
    "        return fuzzy_sets[ordered_sets[0]].lower\n",
    "    elif data > fuzzy_sets[ordered_sets[-1]].upper:\n",
    "        return fuzzy_sets[ordered_sets[-1]].upper\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "def check_bounds(data, fuzzy_sets, ordered_sets):\n",
    "    if data < fuzzy_sets[ordered_sets[0]].lower:\n",
    "        return fuzzy_sets[ordered_sets[0]]\n",
    "    elif data > fuzzy_sets[ordered_sets[-1]].upper:\n",
    "        return fuzzy_sets[ordered_sets[-1]]\n",
    "\n",
    "\n",
    "\n",
    "def check_bounds_index(data, fuzzy_sets, ordered_sets):\n",
    "    if data < fuzzy_sets[ordered_sets[0]].get_lower():\n",
    "        return 0\n",
    "    elif data > fuzzy_sets[ordered_sets[-1]].get_upper():\n",
    "        return len(fuzzy_sets) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Membership functions for Fuzzy Sets\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from pyFTS import *\n",
    "\n",
    "\n",
    "def trimf(x, parameters):\n",
    "    \"\"\"\n",
    "    Triangular fuzzy membership function\n",
    "\n",
    "    :param x: data point\n",
    "    :param parameters: a list with 3 real values\n",
    "    :return: the membership value of x given the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    xx = np.round(x, 3)\n",
    "    if xx < parameters[0]:\n",
    "        return 0\n",
    "    elif parameters[0] <= xx < parameters[1]:\n",
    "        return (x - parameters[0]) / (parameters[1] - parameters[0])\n",
    "    elif parameters[1] <= xx <= parameters[2]:\n",
    "        return (parameters[2] - xx) / (parameters[2] - parameters[1])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def trapmf(x, parameters):\n",
    "    \"\"\"\n",
    "    Trapezoidal fuzzy membership function\n",
    "\n",
    "    :param x: data point\n",
    "    :param parameters: a list with 4 real values\n",
    "    :return: the membership value of x given the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    if x < parameters[0]:\n",
    "        return 0\n",
    "    elif parameters[0] <= x < parameters[1]:\n",
    "        return (x - parameters[0]) / (parameters[1] - parameters[0])\n",
    "    elif parameters[1] <= x <= parameters[2]:\n",
    "        return 1\n",
    "    elif parameters[2] <= x <= parameters[3]:\n",
    "        return (parameters[3] - x) / (parameters[3] - parameters[2])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gaussmf(x, parameters):\n",
    "    \"\"\"\n",
    "    Gaussian fuzzy membership function\n",
    "\n",
    "    :param x: data point\n",
    "    :param parameters: a list with 2 real values (mean and variance)\n",
    "    :return: the membership value of x given the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    return math.exp((-(x - parameters[0])**2)/(2 * parameters[1]**2))\n",
    "\n",
    "\n",
    "\n",
    "def bellmf(x, parameters):\n",
    "    \"\"\"\n",
    "    Bell shaped membership function\n",
    "\n",
    "    :param x:\n",
    "    :param parameters:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + abs((x - parameters[2]) / parameters[0]) ** (2 * parameters[1]))\n",
    "\n",
    "\n",
    "\n",
    "def sigmf(x, parameters):\n",
    "    \"\"\"\n",
    "    Sigmoid / Logistic membership function\n",
    "\n",
    "    :param x:\n",
    "    :param parameters: an list with 2 real values (smoothness and midpoint)\n",
    "    :return\n",
    "    \"\"\"\n",
    "    return 1 / (1 + math.exp(-parameters[0] * (x - parameters[1])))\n",
    "\n",
    "\n",
    "\n",
    "def singleton(x, parameters):\n",
    "    \"\"\"\n",
    "    Singleton membership function, a single value fuzzy function\n",
    "\n",
    "    :param x:\n",
    "    :param parameters: a list with one real value\n",
    "    :returns\n",
    "    \"\"\"\n",
    "    return 1 if x == parameters[0] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyFTS.common import FuzzySet, SortedCollection, tree, Util\n",
    "\n",
    "\n",
    "class FTS(object):\n",
    "    \"\"\"\n",
    "    Fuzzy Time Series object model\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a Fuzzy Time Series model\n",
    "        \"\"\"\n",
    "        self.flrgs = {}\n",
    "        \"\"\"The list of Fuzzy Logical Relationship Groups - FLRG\"\"\"\n",
    "        self.order = kwargs.get('order',1)\n",
    "        \"\"\"A integer with the model order (number of past lags are used on forecasting)\"\"\"\n",
    "        self.shortname = kwargs.get('name',\"\")\n",
    "        \"\"\"A string with a short name or alias for the model\"\"\"\n",
    "        self.name = kwargs.get('name',\"\")\n",
    "        \"\"\"A string with the model name\"\"\"\n",
    "        self.detail = kwargs.get('name',\"\")\n",
    "        \"\"\"A string with the model detailed information\"\"\"\n",
    "        self.is_wrapper = False\n",
    "        \"\"\"Indicates that this model is a wrapper for other(s) method(s)\"\"\"\n",
    "        self.is_high_order = False\n",
    "        \"\"\"A boolean value indicating if the model support orders greater than 1, default: False\"\"\"\n",
    "        self.min_order = 1\n",
    "        \"\"\"In high order models, this integer value indicates the minimal order supported for the model, default: 1\"\"\"\n",
    "        self.has_seasonality = False\n",
    "        \"\"\"A boolean value indicating if the model supports seasonal indexers, default: False\"\"\"\n",
    "        self.has_point_forecasting = True\n",
    "        \"\"\"A boolean value indicating if the model supports point forecasting, default: True\"\"\"\n",
    "        self.has_interval_forecasting = False\n",
    "        \"\"\"A boolean value indicating if the model supports interval forecasting, default: False\"\"\"\n",
    "        self.has_probability_forecasting = False\n",
    "        \"\"\"A boolean value indicating if the model support probabilistic forecasting, default: False\"\"\"\n",
    "        self.is_multivariate = False\n",
    "        \"\"\"A boolean value indicating if the model support multivariate time series (Pandas DataFrame), default: False\"\"\"\n",
    "        self.is_clustered = False\n",
    "        \"\"\"A boolean value indicating if the model support multivariate time series (Pandas DataFrame), but works like \n",
    "        a monovariate method, default: False\"\"\"\n",
    "        self.dump = False\n",
    "        self.transformations = []\n",
    "        \"\"\"A list with the data transformations (common.Transformations) applied on model pre and post processing, default: []\"\"\"\n",
    "        self.transformations_param = []\n",
    "        \"\"\"A list with the specific parameters for each data transformation\"\"\"\n",
    "        self.original_max = 0\n",
    "        \"\"\"A float with the upper limit of the Universe of Discourse, the maximal value found on training data\"\"\"\n",
    "        self.original_min = 0\n",
    "        \"\"\"A float with the lower limit of the Universe of Discourse, the minimal value found on training data\"\"\"\n",
    "        self.partitioner = kwargs.get(\"partitioner\", None)\n",
    "        \"\"\"A pyFTS.partitioners.Partitioner object with the Universe of Discourse partitioner used on the model. This is a mandatory dependecy. \"\"\"\n",
    "        if self.partitioner != None:\n",
    "            self.sets = self.partitioner.sets\n",
    "        self.auto_update = False\n",
    "        \"\"\"A boolean value indicating that model is incremental\"\"\"\n",
    "        self.benchmark_only = False\n",
    "        \"\"\"A boolean value indicating a faÃ§ade for external (non-FTS) model used on benchmarks or ensembles.\"\"\"\n",
    "        self.indexer = kwargs.get(\"indexer\", None)\n",
    "        \"\"\"An pyFTS.models.seasonal.Indexer object for indexing the time series data\"\"\"\n",
    "        self.uod_clip = kwargs.get(\"uod_clip\", True)\n",
    "        \"\"\"Flag indicating if the test data will be clipped inside the training Universe of Discourse\"\"\"\n",
    "        self.alpha_cut = kwargs.get(\"alpha_cut\", 0.0)\n",
    "        \"\"\"A float with the minimal membership to be considered on fuzzyfication process\"\"\"\n",
    "        self.lags = kwargs.get(\"lags\", None)\n",
    "        \"\"\"The list of lag indexes for high order models\"\"\"\n",
    "        self.max_lag = self.order\n",
    "        \"\"\"A integer indicating the largest lag used by the model. This value also indicates the minimum number of past lags \n",
    "        needed to forecast a single step ahead\"\"\"\n",
    "        self.log = pd.DataFrame([],columns=[\"Datetime\",\"Operation\",\"Value\"])\n",
    "        \"\"\"\"\"\"\n",
    "        self.is_time_variant = False\n",
    "        \"\"\"A boolean value indicating if this model is time variant\"\"\"\n",
    "        self.standard_horizon = kwargs.get(\"standard_horizon\", 1)\n",
    "        \"\"\"Standard forecasting horizon (Default: 1)\"\"\"\n",
    "        \n",
    "\n",
    "    def fuzzy(self, data):\n",
    "        \"\"\"\n",
    "        Fuzzify a data point\n",
    "\n",
    "        :param data: data point\n",
    "        :return: maximum membership fuzzy set\n",
    "        \"\"\"\n",
    "        best = {\"fuzzyset\": \"\", \"membership\": 0.0}\n",
    "\n",
    "        for f in self.partitioner.sets:\n",
    "            fset = self.partitioner.sets[f]\n",
    "            if best[\"membership\"] <= fset.membership(data):\n",
    "                best[\"fuzzyset\"] = fset.name\n",
    "                best[\"membership\"] = fset.membership(data)\n",
    "\n",
    "        return best\n",
    "\n",
    "\n",
    "    def clip_uod(self, ndata):\n",
    "        if self.uod_clip and self.partitioner is not None:\n",
    "            ndata = np.clip(ndata, self.partitioner.min, self.partitioner.max)\n",
    "        elif self.uod_clip:\n",
    "            ndata = np.clip(ndata, self.original_min, self.original_max)\n",
    "        return ndata\n",
    "\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Forecast using trained model\n",
    "\n",
    "        :param data: time series with minimal length to the order of the model\n",
    "\n",
    "        :keyword type: the forecasting type, one of these values: point(default), interval, distribution or multivariate.\n",
    "        :keyword steps_ahead: The forecasting path H, i. e., tell the model to forecast from t+1 to t+H.\n",
    "        :keyword step_to: The forecasting step H, i. e., tell the model to forecast to t+H for each input sample \n",
    "        :keyword start_at: in the multi step forecasting, the index of the data where to start forecasting (default value: 0)\n",
    "        :keyword distributed: boolean, indicate if the forecasting procedure will be distributed in a dispy cluster (default value: False)\n",
    "        :keyword nodes: a list with the dispy cluster nodes addresses\n",
    "        :keyword explain: try to explain, step by step, the one-step-ahead point forecasting result given the input data. (default value: False)\n",
    "        :keyword generators: for multivariate methods on multi step ahead forecasting, generators is a dict where the keys\n",
    "                            are the dataframe columun names (except the target_variable) and the values are lambda functions that\n",
    "                            accept one value (the actual value of the variable) and return the next value or trained FTS\n",
    "                            models that accept the actual values and forecast new ones.\n",
    "\n",
    "        :return: a numpy array with the forecasted data\n",
    "        \"\"\"\n",
    "        import copy\n",
    "\n",
    "        kw = copy.deepcopy(kwargs)\n",
    "\n",
    "        if self.is_multivariate:\n",
    "            ndata = data\n",
    "        else:\n",
    "            ndata = self.apply_transformations(data)\n",
    "\n",
    "        ndata = self.clip_uod(ndata)\n",
    "\n",
    "        if 'distributed' in kw:\n",
    "            distributed = kw.pop('distributed')\n",
    "        else:\n",
    "            distributed = False\n",
    "\n",
    "        if 'type' in kw:\n",
    "            type = kw.pop(\"type\")\n",
    "        else:\n",
    "            type = 'point'\n",
    "\n",
    "        if distributed is None or distributed == False:\n",
    "\n",
    "            steps_ahead = kw.get(\"steps_ahead\", None)\n",
    "\n",
    "            step_to = kw.get(\"step_to\", None)\n",
    "\n",
    "            if (steps_ahead == None and step_to == None) or (steps_ahead == 1 or step_to ==1):\n",
    "                if type == 'point':\n",
    "                    ret = self.forecast(ndata, **kw)\n",
    "                elif type == 'interval':\n",
    "                    ret = self.forecast_interval(ndata, **kw)\n",
    "                elif type == 'distribution':\n",
    "                    ret = self.forecast_distribution(ndata, **kw)\n",
    "                elif type == 'multivariate':\n",
    "                    ret = self.forecast_multivariate(ndata, **kw)\n",
    "            elif step_to == None and steps_ahead > 1:\n",
    "                if type == 'point':\n",
    "                    ret = self.forecast_ahead(ndata, steps_ahead, **kw)\n",
    "                elif type == 'interval':\n",
    "                    ret = self.forecast_ahead_interval(ndata, steps_ahead, **kw)\n",
    "                elif type == 'distribution':\n",
    "                    ret = self.forecast_ahead_distribution(ndata, steps_ahead, **kw)\n",
    "                elif type == 'multivariate':\n",
    "                    ret = self.forecast_ahead_multivariate(ndata, steps_ahead, **kw)\n",
    "            elif step_to > 1:\n",
    "                if type == 'point':\n",
    "                    ret = self.forecast_step(ndata, step_to, **kw)\n",
    "                else:\n",
    "                    raise NotImplementedError('This model only perform point step ahead forecasts!')\n",
    "\n",
    "            if not ['point', 'interval', 'distribution', 'multivariate'].__contains__(type):\n",
    "                raise ValueError('The argument \\'type\\' has an unknown value.')\n",
    "\n",
    "        else:\n",
    "\n",
    "            if distributed == 'dispy':\n",
    "                from pyFTS.distributed import dispy\n",
    "\n",
    "                nodes = kw.pop(\"nodes\", ['127.0.0.1'])\n",
    "                num_batches = kw.pop('num_batches', 10)\n",
    "\n",
    "                ret = dispy.distributed_predict(self, kw, nodes, ndata, num_batches, **kw)\n",
    "\n",
    "            elif distributed == 'spark':\n",
    "                from pyFTS.distributed import spark\n",
    "\n",
    "                ret = spark.distributed_predict(data=ndata, model=self, **kw)\n",
    "\n",
    "        if not self.is_multivariate:\n",
    "            kw['type'] = type\n",
    "            ret = self.apply_inverse_transformations(ret, params=[data[self.max_lag - 1:]], **kw)\n",
    "\n",
    "        if 'statistics' in kw:\n",
    "            kwargs['statistics'] = kw['statistics']\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def forecast(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Point forecast one step ahead\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param kwargs: model specific parameters\n",
    "        :return: a list with the forecasted values\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform one step ahead point forecasts!')\n",
    "\n",
    "\n",
    "    def forecast_interval(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Interval forecast one step ahead\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param kwargs: model specific parameters\n",
    "        :return: a list with the prediction intervals\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform one step ahead interval forecasts!')\n",
    "\n",
    "\n",
    "    def forecast_distribution(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Probabilistic forecast one step ahead\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param kwargs: model specific parameters\n",
    "        :return: a list with probabilistic.ProbabilityDistribution objects representing the forecasted Probability Distributions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform one step ahead distribution forecasts!')\n",
    "\n",
    "\n",
    "    def forecast_multivariate(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Multivariate forecast one step ahead\n",
    "\n",
    "        :param data: Pandas dataframe with one column for each variable and with the minimal length equal to the max_lag of the model\n",
    "        :param kwargs: model specific parameters\n",
    "        :return: a Pandas Dataframe object representing the forecasted values for each variable\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform one step ahead multivariate forecasts!')\n",
    "\n",
    "\n",
    "\n",
    "    def forecast_ahead(self, data, steps, **kwargs):\n",
    "        \"\"\"\n",
    "        Point forecast from 1 to H steps ahead, where H is given by the steps parameter\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param steps: the number of steps ahead to forecast (default: 1)\n",
    "        :keyword start_at: in the multi step forecasting, the index of the data where to start forecasting (default: 0)\n",
    "        :return: a list with the forecasted values\n",
    "        \"\"\"\n",
    "\n",
    "        if len(data) < self.max_lag:\n",
    "            return data\n",
    "\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = data.tolist()\n",
    "\n",
    "        start = kwargs.get('start_at',0)\n",
    "\n",
    "        ret = data[:start+self.max_lag]\n",
    "        for k in np.arange(start+self.max_lag, steps+start+self.max_lag):\n",
    "            tmp = self.forecast(ret[k-self.max_lag:k], **kwargs)\n",
    "\n",
    "            if isinstance(tmp,(list, np.ndarray)):\n",
    "                tmp = tmp[-1]\n",
    "\n",
    "            ret.append(tmp)\n",
    "            data.append(tmp)\n",
    "\n",
    "        return ret[-steps:]\n",
    "\n",
    "\n",
    "    def forecast_ahead_interval(self, data, steps, **kwargs):\n",
    "        \"\"\"\n",
    "        Interval forecast from 1 to H steps ahead, where H is given by the steps parameter\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param steps: the number of steps ahead to forecast\n",
    "        :keyword start_at: in the multi step forecasting, the index of the data where to start forecasting (default: 0)\n",
    "        :return: a list with the forecasted intervals\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform multi step ahead interval forecasts!')\n",
    "\n",
    "\n",
    "    def forecast_ahead_distribution(self, data, steps, **kwargs):\n",
    "        \"\"\"\n",
    "        Probabilistic forecast from 1 to H steps ahead, where H is given by the steps parameter\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param steps: the number of steps ahead to forecast\n",
    "        :keyword start_at: in the multi step forecasting, the index of the data where to start forecasting (default: 0)\n",
    "        :return: a list with the forecasted Probability Distributions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform multi step ahead distribution forecasts!')\n",
    "\n",
    "\n",
    "    def forecast_ahead_multivariate(self, data, steps, **kwargs):\n",
    "        \"\"\"\n",
    "        Multivariate forecast n step ahead\n",
    "\n",
    "        :param data: Pandas dataframe with one column for each variable and with the minimal length equal to the max_lag of the model\n",
    "        :param steps: the number of steps ahead to forecast\n",
    "        :keyword start_at: in the multi step forecasting, the index of the data where to start forecasting (default: 0)\n",
    "        :return: a Pandas Dataframe object representing the forecasted values for each variable\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This model do not perform one step ahead multivariate forecasts!')\n",
    "\n",
    "\n",
    "    def forecast_step(self, data, step, **kwargs):\n",
    "        \"\"\"\n",
    "        Point forecast for H steps ahead, where H is given by the step parameter\n",
    "\n",
    "        :param data: time series data with the minimal length equal to the max_lag of the model\n",
    "        :param step: the forecasting horizon (default: 1)\n",
    "        :keyword start_at: in the multi step forecasting, the index of the data where to start forecasting (default: 0)\n",
    "        :return: a list with the forecasted values\n",
    "        \"\"\"\n",
    "\n",
    "        l = len(data)\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        if l < self.max_lag:\n",
    "            return data\n",
    "\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = data.tolist()\n",
    "\n",
    "        start = kwargs.get('start_at',0)\n",
    "\n",
    "        for k in np.arange(start+self.max_lag, l):\n",
    "            sample = data[k-self.max_lag:k]\n",
    "            tmp = self.forecast_ahead(sample, step, **kwargs)\n",
    "\n",
    "            if isinstance(tmp,(list, np.ndarray)):\n",
    "                tmp = tmp[-1]\n",
    "\n",
    "            ret.append(tmp)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def train(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Method specific parameter fitting\n",
    "\n",
    "        :param data: training time series data\n",
    "        :param kwargs: Method specific parameters\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, ndata, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit the model's parameters based on the training data.\n",
    "\n",
    "        :param ndata: training time series data\n",
    "        :param kwargs:\n",
    "\n",
    "        :keyword num_batches: split the training data in num_batches to save memory during the training process\n",
    "        :keyword save_model: save final model on disk\n",
    "        :keyword batch_save: save the model between each batch\n",
    "        :keyword file_path: path to save the model\n",
    "        :keyword distributed: boolean, indicate if the training procedure will be distributed in a dispy cluster\n",
    "        :keyword nodes: a list with the dispy cluster nodes addresses\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        import datetime, copy\n",
    "\n",
    "        kw = copy.deepcopy(kwargs)\n",
    "\n",
    "        if self.is_multivariate:\n",
    "            data = ndata\n",
    "        else:\n",
    "            data = self.apply_transformations(ndata)\n",
    "\n",
    "            self.original_min = np.nanmin(data)\n",
    "            self.original_max = np.nanmax(data)\n",
    "\n",
    "        if 'partitioner' in kw:\n",
    "            self.partitioner = kw.pop('partitioner')\n",
    "\n",
    "        if not self.is_multivariate and not self.is_wrapper and not self.benchmark_only:\n",
    "            if self.partitioner is None:\n",
    "                raise Exception(\"Fuzzy sets were not provided for the model. Use 'partitioner' parameter. \")\n",
    "\n",
    "        if 'order' in kw:\n",
    "            self.order = kw.pop('order')\n",
    "\n",
    "        dump = kw.get('dump', None)\n",
    "\n",
    "        num_batches = kw.pop('num_batches', None)\n",
    "\n",
    "        save = kw.get('save_model', False)  # save model on disk\n",
    "\n",
    "        batch_save = kw.get('batch_save', False) #save model between batches\n",
    "\n",
    "        file_path = kw.get('file_path', None)\n",
    "\n",
    "        distributed = kw.pop('distributed', False)\n",
    "\n",
    "        if distributed is not None and distributed:\n",
    "            if num_batches is None:\n",
    "                num_batches = 10\n",
    "\n",
    "            if distributed == 'dispy':\n",
    "                from pyFTS.distributed import dispy\n",
    "                nodes = kw.pop('nodes', False)\n",
    "                train_method = kwargs.get('train_method', dispy.simple_model_train)\n",
    "                dispy.distributed_train(self, train_method, nodes, type(self), data, num_batches, {},\n",
    "                                       **kw)\n",
    "            elif distributed == 'spark':\n",
    "                from pyFTS.distributed import spark\n",
    "                url = kwargs.get('url', 'spark://127.0.0.1:7077')\n",
    "                app = kwargs.get('app', 'pyFTS')\n",
    "\n",
    "                spark.distributed_train(self, data, url=url, app=app)\n",
    "        else:\n",
    "\n",
    "            if dump == 'time':\n",
    "                print(\"[{0: %H:%M:%S}] Start training\".format(datetime.datetime.now()))\n",
    "\n",
    "            if num_batches is not None and not self.is_wrapper:\n",
    "                n = len(data)\n",
    "                batch_size = int(n / num_batches)\n",
    "                bcount = 1\n",
    "\n",
    "                rng = range(self.order, n, batch_size)\n",
    "\n",
    "                if dump == 'tqdm':\n",
    "                    from tqdm import tqdm\n",
    "\n",
    "                    rng = tqdm(rng)\n",
    "\n",
    "                for ct in rng:\n",
    "                    if dump == 'time':\n",
    "                        print(\"[{0: %H:%M:%S}] Starting batch \".format(datetime.datetime.now()) + str(bcount))\n",
    "                    if self.is_multivariate:\n",
    "                        mdata = data.iloc[ct - self.order:ct + batch_size]\n",
    "                    else:\n",
    "                        mdata = data[ct - self.order : ct + batch_size]\n",
    "\n",
    "                    self.train(mdata, **kw)\n",
    "\n",
    "                    if batch_save:\n",
    "                        Util.persist_obj(self,file_path)\n",
    "\n",
    "                    if dump == 'time':\n",
    "                        print(\"[{0: %H:%M:%S}] Finish batch \".format(datetime.datetime.now()) + str(bcount))\n",
    "\n",
    "                    bcount += 1\n",
    "\n",
    "            else:\n",
    "                self.train(data, **kw)\n",
    "\n",
    "            if dump == 'time':\n",
    "                print(\"[{0: %H:%M:%S}] Finish training\".format(datetime.datetime.now()))\n",
    "\n",
    "        if save:\n",
    "            Util.persist_obj(self, file_path)\n",
    "\n",
    "\n",
    "\n",
    "    def clone_parameters(self, model):\n",
    "        \"\"\"\n",
    "        Import the parameters values from other model\n",
    "\n",
    "        :param model: a model to clone the parameters\n",
    "        \"\"\"\n",
    "\n",
    "        self.order = model.order\n",
    "        self.partitioner = model.partitioner\n",
    "        self.lags = model.lags\n",
    "        self.shortname = model.shortname\n",
    "        self.name = model.name\n",
    "        self.detail = model.detail\n",
    "        self.is_high_order = model.is_high_order\n",
    "        self.min_order = model.min_order\n",
    "        self.has_seasonality = model.has_seasonality\n",
    "        self.has_point_forecasting = model.has_point_forecasting\n",
    "        self.has_interval_forecasting = model.has_interval_forecasting\n",
    "        self.has_probability_forecasting = model.has_probability_forecasting\n",
    "        self.is_multivariate = model.is_multivariate\n",
    "        self.dump = model.dump\n",
    "        self.transformations = model.transformations\n",
    "        self.transformations_param = model.transformations_param\n",
    "        self.original_max = model.original_max\n",
    "        self.original_min = model.original_min\n",
    "        self.auto_update = model.auto_update\n",
    "        self.benchmark_only = model.benchmark_only\n",
    "        self.indexer = model.indexer\n",
    "\n",
    "\n",
    "    def append_rule(self, flrg):\n",
    "        \"\"\"\n",
    "        Append FLRG rule to the model\n",
    "\n",
    "        :param flrg: rule\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if flrg.get_key() not in self.flrgs:\n",
    "            self.flrgs[flrg.get_key()] = flrg\n",
    "        else:\n",
    "            if isinstance(flrg.RHS, (list, set)):\n",
    "                for k in flrg.RHS:\n",
    "                    self.flrgs[flrg.get_key()].append_rhs(k)\n",
    "            elif isinstance(flrg.RHS, dict):\n",
    "                for key, value in flrg.RHS.items():\n",
    "                    self.flrgs[flrg.get_key()].append_rhs(key, count=value)\n",
    "            else:\n",
    "                self.flrgs[flrg.get_key()].append_rhs(flrg.RHS)\n",
    "\n",
    "\n",
    "    def merge(self, model):\n",
    "        \"\"\"\n",
    "        Merge the FLRG rules from other model\n",
    "\n",
    "        :param model: source model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        for key, flrg in model.flrgs.items():\n",
    "            self.append_rule(flrg)\n",
    "\n",
    "\n",
    "    def append_transformation(self, transformation):\n",
    "        if transformation is not None:\n",
    "            self.transformations.append(transformation)\n",
    "\n",
    "\n",
    "    def apply_transformations(self, data, params=None, updateUoD=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Apply the data transformations for data preprocessing\n",
    "\n",
    "        :param data: input data\n",
    "        :param params: transformation parameters\n",
    "        :param updateUoD:\n",
    "        :param kwargs:\n",
    "        :return: preprocessed data\n",
    "        \"\"\"\n",
    "\n",
    "        ndata = data\n",
    "        if updateUoD:\n",
    "            if min(data) < 0:\n",
    "                self.original_min = min(data) * 1.1\n",
    "            else:\n",
    "                self.original_min = min(data) * 0.9\n",
    "\n",
    "            if max(data) > 0:\n",
    "                self.original_max = max(data) * 1.1\n",
    "            else:\n",
    "                self.original_max = max(data) * 0.9\n",
    "\n",
    "        if len(self.transformations) > 0:\n",
    "            if params is None:\n",
    "                params = [ None for k in self.transformations]\n",
    "\n",
    "            for c, t in enumerate(self.transformations, start=0):\n",
    "                ndata = t.apply(ndata, params[c], )\n",
    "\n",
    "        return ndata\n",
    "\n",
    "\n",
    "    def apply_inverse_transformations(self, data, params=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Apply the data transformations for data postprocessing\n",
    "\n",
    "        :param data: input data\n",
    "        :param params: transformation parameters\n",
    "        :param updateUoD:\n",
    "        :param kwargs:\n",
    "        :return: postprocessed data\n",
    "        \"\"\"\n",
    "        if len(self.transformations) > 0:\n",
    "            if params is None:\n",
    "                params = [None for k in self.transformations]\n",
    "\n",
    "            for c, t in enumerate(reversed(self.transformations), start=0):\n",
    "                ndata = t.inverse(data, params[c], **kwargs)\n",
    "\n",
    "            return ndata\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "\n",
    "    def get_UoD(self):\n",
    "        \"\"\"\n",
    "        Returns the interval of the known bounds of the universe of discourse (UoD), i. e.,\n",
    "        the known minimum and maximum values of the time series.\n",
    "\n",
    "        :return: A set with the lower and the upper bounds of the UoD\n",
    "        \"\"\"\n",
    "        if self.partitioner is not None:\n",
    "            return (self.partitioner.min, self.partitioner.max)\n",
    "        else:\n",
    "            return (self.original_min, self.original_max)\n",
    "\n",
    "\n",
    "    def offset(self):\n",
    "        \"\"\"\n",
    "        Returns the number of lags to skip in the input test data in order to synchronize it with\n",
    "        the forecasted values given by the predict function. This is necessary due to the order of the\n",
    "        model, among other parameters.\n",
    "\n",
    "        :return: An integer with the number of lags to skip\n",
    "        \"\"\"\n",
    "        if self.is_high_order:\n",
    "            return self.max_lag\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the model\n",
    "\n",
    "        :return: a string containing the name of the model and the learned rules\n",
    "        (if the model was already trained)\n",
    "        \"\"\"\n",
    "\n",
    "        tmp = self.name + \":\\n\"\n",
    "        if self.partitioner.type == 'common':\n",
    "            for r in sorted(self.flrgs, key=lambda key: self.flrgs[key].get_midpoint(self.partitioner.sets)):\n",
    "                tmp = \"{0}{1}\\n\".format(tmp, str(self.flrgs[r]))\n",
    "        else:\n",
    "            for r in self.flrgs:\n",
    "                tmp = \"{0}{1}\\n\".format(tmp, str(self.flrgs[r]))\n",
    "        return tmp\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The length (number of rules) of the model\n",
    "\n",
    "        :return: number of rules\n",
    "        \"\"\"\n",
    "        return len(self.flrgs)\n",
    "\n",
    "    def len_total(self):\n",
    "        \"\"\"\n",
    "        Total length of the model, adding the number of terms in all rules\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return sum([len(k) for k in self.flrgs])\n",
    "\n",
    "\n",
    "    def reset_calculated_values(self):\n",
    "        \"\"\"\n",
    "        Reset all pre-calculated values on the FLRG's\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        for flrg in self.flrgs.keys():\n",
    "            self.flrgs[flrg].reset_calculated_values()\n",
    "\n",
    "\n",
    "    def append_log(self,operation, value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyFTS.common import FuzzySet\n",
    "\n",
    "\n",
    "class FLR(object):\n",
    "    \"\"\"\n",
    "    Fuzzy Logical Relationship\n",
    "\n",
    "    Represents a temporal transition of the fuzzy set LHS on time t for the fuzzy set RHS on time t+1.\n",
    "    \"\"\"\n",
    "    def __init__(self, LHS, RHS):\n",
    "        \"\"\"\n",
    "        Creates a Fuzzy Logical Relationship\n",
    "        \"\"\"\n",
    "        self.LHS = LHS\n",
    "        \"\"\"Left Hand Side fuzzy set\"\"\"\n",
    "        self.RHS = RHS\n",
    "        \"\"\"Right Hand Side fuzzy set\"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.LHS) + \" -> \" + str(self.RHS)\n",
    "\n",
    "\n",
    "\n",
    "class IndexedFLR(FLR):\n",
    "    \"\"\"Season Indexed Fuzzy Logical Relationship\"\"\"\n",
    "    def __init__(self, index, LHS, RHS):\n",
    "        \"\"\"\n",
    "        Create a Season Indexed Fuzzy Logical Relationship\n",
    "        \"\"\"\n",
    "        super(IndexedFLR, self).__init__(LHS, RHS)\n",
    "        self.index = index\n",
    "        \"\"\"seasonal index\"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.index) + \": \"+ str(self.LHS) + \" -> \" + str(self.RHS)\n",
    "\n",
    "\n",
    "\n",
    "def generate_high_order_recurrent_flr(fuzzyData):\n",
    "    \"\"\"\n",
    "    Create a ordered FLR set from a list of fuzzy sets with recurrence\n",
    "\n",
    "    :param fuzzyData: ordered list of fuzzy sets\n",
    "    :return: ordered list of FLR\n",
    "    \"\"\"\n",
    "    flrs = []\n",
    "    for i in np.arange(1,len(fuzzyData)):\n",
    "        lhs = fuzzyData[i - 1]\n",
    "        rhs = fuzzyData[i]\n",
    "        if isinstance(lhs, list) and isinstance(rhs, list):\n",
    "            for l in lhs:\n",
    "                for r in rhs:\n",
    "                    tmp = FLR(l, r)\n",
    "                    flrs.append(tmp)\n",
    "        else:\n",
    "            tmp = FLR(lhs,rhs)\n",
    "            flrs.append(tmp)\n",
    "    return flrs\n",
    "\n",
    "\n",
    "\n",
    "def generate_recurrent_flrs(fuzzyData, steps = 1):\n",
    "    \"\"\"\n",
    "    Create a ordered FLR set from a list of fuzzy sets with recurrence\n",
    "\n",
    "    :param fuzzyData: ordered list of fuzzy sets\n",
    "    :param steps: the number of steps ahead on the right side of FLR\n",
    "    :return: ordered list of FLR\n",
    "    \"\"\"\n",
    "    _tmp_steps = steps - 1\n",
    "    flrs = []\n",
    "    for i in np.arange(1,len(fuzzyData) - _tmp_steps):\n",
    "        lhs = [fuzzyData[i - 1]]\n",
    "        rhs = [fuzzyData[i+_tmp_steps]]\n",
    "        for l in np.array(lhs).flatten():\n",
    "            for r in np.array(rhs).flatten():\n",
    "                tmp = FLR(l, r)\n",
    "                flrs.append(tmp)\n",
    "    return flrs\n",
    "\n",
    "\n",
    "\n",
    "def generate_non_recurrent_flrs(fuzzyData, steps = 1):\n",
    "    \"\"\"\n",
    "    Create a ordered FLR set from a list of fuzzy sets without recurrence\n",
    "\n",
    "    :param fuzzyData: ordered list of fuzzy sets\n",
    "    :return: ordered list of FLR\n",
    "    \"\"\"\n",
    "    flrs = generate_recurrent_flrs(fuzzyData, steps=steps)\n",
    "    tmp = {}\n",
    "    for flr in flrs: tmp[str(flr)] = flr\n",
    "    ret = [value for key, value in tmp.items()]\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def generate_indexed_flrs(sets, indexer, data, transformation=None, alpha_cut=0.0):\n",
    "    \"\"\"\n",
    "    Create a season-indexed ordered FLR set from a list of fuzzy sets with recurrence\n",
    "\n",
    "    :param sets: fuzzy sets\n",
    "    :param indexer: seasonality indexer \n",
    "    :param data: original data\n",
    "    :return: ordered list of FLR \n",
    "    \"\"\"\n",
    "    flrs = []\n",
    "    index = indexer.get_season_of_data(data)\n",
    "    ndata = indexer.get_data(data)\n",
    "    if transformation is not None:\n",
    "        ndata = transformation.apply(ndata)\n",
    "    for k in np.arange(1,len(ndata)):\n",
    "        lhs = FuzzySet.fuzzyfy_series([ndata[k - 1]], sets, method='fuzzy',alpha_cut=alpha_cut)\n",
    "        rhs = FuzzySet.fuzzyfy_series([ndata[k]], sets, method='fuzzy',alpha_cut=alpha_cut)\n",
    "        season = index[k]\n",
    "        for _l in np.array(lhs).flatten():\n",
    "            for _r in np.array(rhs).flatten():\n",
    "                flr = IndexedFLR(season,_l,_r)\n",
    "                flrs.append(flr)\n",
    "    return flrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class FLRG(object):\n",
    "    \"\"\"\n",
    "    Fuzzy Logical Relationship Group\n",
    "\n",
    "    Group a set of FLR's with the same LHS. Represents the temporal patterns for time t+1 (the RHS fuzzy sets)\n",
    "    when the LHS pattern is identified on time t.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, order, **kwargs):\n",
    "        self.LHS = None\n",
    "        \"\"\"Left Hand Side of the rule\"\"\"\n",
    "        self.RHS = None\n",
    "        \"\"\"Right Hand Side of the rule\"\"\"\n",
    "        self.order = order\n",
    "        \"\"\"Number of lags on LHS\"\"\"\n",
    "        self.midpoint = None\n",
    "        self.lower = None\n",
    "        self.upper = None\n",
    "        self.key = None\n",
    "\n",
    "    def append_rhs(self, set, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_key(self):\n",
    "        \"\"\"Returns a unique identifier for this FLRG\"\"\"\n",
    "        if self.key is None:\n",
    "            if isinstance(self.LHS, (list, set)):\n",
    "                names = [c for c in self.LHS]\n",
    "            elif isinstance(self.LHS, dict):\n",
    "                names = [self.LHS[k] for k in self.LHS.keys()]\n",
    "            else:\n",
    "                names = [self.LHS]\n",
    "\n",
    "            self.key = \"\"\n",
    "\n",
    "            for n in names:\n",
    "                if len(self.key) > 0:\n",
    "                    self.key += \",\"\n",
    "                self.key += n\n",
    "        return self.key\n",
    "\n",
    "\n",
    "    def get_membership(self, data, sets):\n",
    "        \"\"\"\n",
    "        Returns the membership value of the FLRG for the input data\n",
    "\n",
    "        :param data: input data\n",
    "        :param sets: fuzzy sets\n",
    "        :return: the membership value\n",
    "        \"\"\"\n",
    "        ret = 0.0\n",
    "        if isinstance(self.LHS, (list, set)):\n",
    "            if len(self.LHS) == len(data):\n",
    "                ret = np.nanmin([sets[self.LHS[ct]].membership(dat) for ct, dat in enumerate(data)])\n",
    "        else:\n",
    "            ret = sets[self.LHS].membership(data)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_midpoint(self, sets):\n",
    "        \"\"\"\n",
    "        Returns the midpoint value for the RHS fuzzy sets\n",
    "\n",
    "        :param sets: fuzzy sets\n",
    "        :return: the midpoint value\n",
    "        \"\"\"\n",
    "        if self.midpoint is None:\n",
    "            self.midpoint = np.nanmean(self.get_midpoints(sets))\n",
    "        return self.midpoint\n",
    "\n",
    "\n",
    "    def get_midpoints(self, sets):\n",
    "        if isinstance(self.RHS, (list, set)):\n",
    "            return np.array([sets[s].centroid for s in self.RHS])\n",
    "        elif isinstance(self.RHS, dict):\n",
    "            return np.array([sets[s].centroid for s in self.RHS.keys()])\n",
    "\n",
    "\n",
    "    def get_lower(self, sets):\n",
    "        \"\"\"\n",
    "        Returns the lower bound value for the RHS fuzzy sets\n",
    "\n",
    "        :param sets: fuzzy sets\n",
    "        :return: lower bound value\n",
    "        \"\"\"\n",
    "        if self.lower is None:\n",
    "            if isinstance(self.RHS, list):\n",
    "                self.lower = min([sets[rhs].lower for rhs in self.RHS])\n",
    "            elif isinstance(self.RHS, dict):\n",
    "                self.lower = min([sets[self.RHS[s]].lower for s in self.RHS.keys()])\n",
    "        return self.lower\n",
    "\n",
    "\n",
    "    def get_upper(self, sets):\n",
    "        \"\"\"\n",
    "        Returns the upper bound value for the RHS fuzzy sets\n",
    "\n",
    "        :param sets: fuzzy sets\n",
    "        :return: upper bound value\n",
    "        \"\"\"\n",
    "        if self.upper is None:\n",
    "            if isinstance(self.RHS, list):\n",
    "                self.upper = max([sets[rhs].upper for rhs in self.RHS])\n",
    "            elif isinstance(self.RHS, dict):\n",
    "                self.upper = max([sets[self.RHS[s]].upper for s in self.RHS.keys()])\n",
    "        return self.upper\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.RHS)\n",
    "\n",
    "    def reset_calculated_values(self):\n",
    "        self.midpoint = None\n",
    "        self.upper = None\n",
    "        self.lower = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.common import fts, FuzzySet, FLR, Membership\n",
    "from pyFTS.partitioners import Grid\n",
    "from pyFTS.models.multivariate import FLR as MVFLR, common, flrg as mvflrg\n",
    "from itertools import product\n",
    "from types import LambdaType\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def product_dict(**kwargs):\n",
    "    \"\"\"\n",
    "    Code by Seth Johnson\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "\n",
    "\n",
    "class MVFTS(fts.FTS):\n",
    "    \"\"\"\n",
    "    Multivariate extension of Chen's ConventionalFTS method\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MVFTS, self).__init__(**kwargs)\n",
    "        self.explanatory_variables = kwargs.get('explanatory_variables',[])\n",
    "        self.target_variable = kwargs.get('target_variable',None)\n",
    "        self.flrgs = {}\n",
    "        self.is_multivariate = True\n",
    "        self.shortname = \"MVFTS\"\n",
    "        self.name = \"Multivariate FTS\"\n",
    "        self.uod_clip = False\n",
    "\n",
    "    def append_transformation(self, transformation, **kwargs):\n",
    "        if not transformation.is_multivariate:\n",
    "            raise Exception('The transformation is not multivariate')\n",
    "        self.transformations.append(transformation)\n",
    "        self.transformations_param.append(kwargs)\n",
    "\n",
    "\n",
    "    def append_variable(self, var):\n",
    "        \"\"\"\n",
    "        Append a new endogenous variable to the model\n",
    "\n",
    "        :param var: variable object\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.explanatory_variables.append(var)\n",
    "\n",
    "\n",
    "    def format_data(self, data):\n",
    "        ndata = {}\n",
    "        for var in self.explanatory_variables:\n",
    "            ndata[var.name] = var.partitioner.extractor(data[var.data_label])\n",
    "\n",
    "        return ndata\n",
    "\n",
    "\n",
    "    def apply_transformations(self, data, params=None, updateUoD=False, **kwargs):\n",
    "        ndata = data.copy(deep=True)\n",
    "        for ct, transformation in enumerate(self.transformations):\n",
    "            ndata = transformation.apply(ndata, **self.transformations_param[ct])\n",
    "\n",
    "        for var in self.explanatory_variables:\n",
    "            try:\n",
    "                values = ndata[var.data_label].values #if isinstance(ndata, pd.DataFrame) else ndata[var.data_label]\n",
    "                if self.uod_clip and var.partitioner.type == 'common':\n",
    "                    ndata[var.data_label] = np.clip(values,\n",
    "                                                    var.partitioner.min, var.partitioner.max)\n",
    "\n",
    "                ndata[var.data_label] = var.apply_transformations(values)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return ndata\n",
    "\n",
    "\n",
    "    def generate_lhs_flrs(self, data):\n",
    "        flrs = []\n",
    "        lags = {}\n",
    "        for vc, var in enumerate(self.explanatory_variables):\n",
    "            data_point = data[var.name]\n",
    "            lags[var.name] = common.fuzzyfy_instance(data_point, var, tuples=False)\n",
    "\n",
    "        for path in product_dict(**lags):\n",
    "            flr = MVFLR.FLR()\n",
    "\n",
    "            flr.LHS = path\n",
    "\n",
    "            #for var, fset in path.items():\n",
    "            #    flr.set_lhs(var, fset)\n",
    "\n",
    "            if len(flr.LHS.keys()) == len(self.explanatory_variables):\n",
    "                flrs.append(flr)\n",
    "\n",
    "        return flrs\n",
    "\n",
    "\n",
    "    def generate_flrs(self, data):\n",
    "        flrs = []\n",
    "        for ct in np.arange(1, len(data.index)):\n",
    "            ix = data.index[ct-1]\n",
    "            data_point = self.format_data( data.loc[ix] )\n",
    "\n",
    "            tmp_flrs = self.generate_lhs_flrs(data_point)\n",
    "\n",
    "            target_ix = data.index[ct]\n",
    "            target_point = data[self.target_variable.data_label][target_ix]\n",
    "            target = common.fuzzyfy_instance(target_point, self.target_variable)\n",
    "\n",
    "            for flr in tmp_flrs:\n",
    "                for v, s in target:\n",
    "                    new_flr = deepcopy(flr)\n",
    "                    new_flr.set_rhs(s)\n",
    "                    flrs.append(new_flr)\n",
    "\n",
    "        return flrs\n",
    "\n",
    "\n",
    "    def generate_flrg(self, flrs):\n",
    "        for flr in flrs:\n",
    "            flrg = mvflrg.FLRG(lhs=flr.LHS)\n",
    "\n",
    "            if flrg.get_key() not in self.flrgs:\n",
    "                self.flrgs[flrg.get_key()] = flrg\n",
    "\n",
    "            self.flrgs[flrg.get_key()].append_rhs(flr.RHS)\n",
    "\n",
    "\n",
    "    def train(self, data, **kwargs):\n",
    "\n",
    "        ndata = self.apply_transformations(data)\n",
    "\n",
    "        flrs = self.generate_flrs(ndata)\n",
    "        self.generate_flrg(flrs)\n",
    "\n",
    "\n",
    "    def forecast(self, data, **kwargs):\n",
    "        ret = []\n",
    "        ndata = self.apply_transformations(data)\n",
    "        c = 0\n",
    "        for index, row in ndata.iterrows() if isinstance(ndata, pd.DataFrame) else enumerate(ndata):\n",
    "            data_point = self.format_data(row)\n",
    "            flrs = self.generate_lhs_flrs(data_point)\n",
    "            mvs = []\n",
    "            mps = []\n",
    "            for flr in flrs:\n",
    "                flrg = mvflrg.FLRG(lhs=flr.LHS)\n",
    "                if flrg.get_key() not in self.flrgs:\n",
    "                    #NaÃ¯ve approach is applied when no rules were found\n",
    "                    if self.target_variable.name in flrg.LHS:\n",
    "                        fs = flrg.LHS[self.target_variable.name]\n",
    "                        fset = self.target_variable.partitioner.sets[fs]\n",
    "                        mp = fset.centroid\n",
    "                        mv = fset.membership(data_point[self.target_variable.name])\n",
    "                        mvs.append(mv)\n",
    "                        mps.append(mp)\n",
    "                    else:\n",
    "                        mvs.append(0.)\n",
    "                        mps.append(0.)\n",
    "                else:\n",
    "                    _flrg = self.flrgs[flrg.get_key()]\n",
    "                    mvs.append(_flrg.get_membership(data_point, self.explanatory_variables))\n",
    "                    mps.append(_flrg.get_midpoint(self.target_variable.partitioner.sets))\n",
    "\n",
    "            mv = np.array(mvs)\n",
    "            mp = np.array(mps)\n",
    "\n",
    "            ret.append(np.dot(mv,mp.T)/np.nansum(mv))\n",
    "\n",
    "        ret = self.target_variable.apply_inverse_transformations(ret,\n",
    "                                                           params=data[self.target_variable.data_label].values)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def forecast_ahead(self, data, steps, **kwargs):\n",
    "        generators = kwargs.get('generators',None)\n",
    "\n",
    "        if generators is None:\n",
    "            raise Exception('You must provide parameter \\'generators\\'! generators is a dict where the keys' +\n",
    "                            ' are the dataframe column names (except the target_variable) and the values are ' +\n",
    "                            'lambda functions that accept one value (the actual value of the variable) '\n",
    "                            ' and return the next value or trained FTS models that accept the actual values and '\n",
    "                            'forecast new ones.')\n",
    "\n",
    "        ndata = self.apply_transformations(data)\n",
    "\n",
    "        start = kwargs.get('start_at', 0)\n",
    "\n",
    "        ndata = ndata.iloc[start: start + self.max_lag]\n",
    "        ret = []\n",
    "        for k in np.arange(0, steps):\n",
    "            sample = ndata.iloc[-self.max_lag:]\n",
    "            tmp = self.forecast(sample, **kwargs)\n",
    "\n",
    "            if isinstance(tmp, (list, np.ndarray)):\n",
    "                tmp = tmp[-1]\n",
    "\n",
    "            ret.append(tmp)\n",
    "\n",
    "            new_data_point = {}\n",
    "\n",
    "            for data_label in generators.keys():\n",
    "                if data_label != self.target_variable.data_label:\n",
    "                    if isinstance(generators[data_label], LambdaType):\n",
    "                        last_data_point = ndata.loc[ndata.index[-1]]\n",
    "                        new_data_point[data_label] = generators[data_label](last_data_point[data_label])\n",
    "                    elif isinstance(generators[data_label], fts.FTS):\n",
    "                        gen_model = generators[data_label]\n",
    "                        last_data_point = sample.iloc[-gen_model.order:]\n",
    "\n",
    "                        if not gen_model.is_multivariate:\n",
    "                            last_data_point = last_data_point[data_label].values\n",
    "\n",
    "                        new_data_point[data_label] = gen_model.forecast(last_data_point)[0]\n",
    "\n",
    "            new_data_point[self.target_variable.data_label] = tmp\n",
    "\n",
    "            ndata = ndata.append(new_data_point, ignore_index=True)\n",
    "\n",
    "        return ret[-steps:]\n",
    "\n",
    "\n",
    "    def forecast_interval(self, data, **kwargs):\n",
    "        ret = []\n",
    "        ndata = self.apply_transformations(data)\n",
    "        c = 0\n",
    "        for index, row in ndata.iterrows() if isinstance(ndata, pd.DataFrame) else enumerate(ndata):\n",
    "            data_point = self.format_data(row)\n",
    "            flrs = self.generate_lhs_flrs(data_point)\n",
    "            mvs = []\n",
    "            ups = []\n",
    "            los = []\n",
    "            for flr in flrs:\n",
    "                flrg = mvflrg.FLRG(lhs=flr.LHS)\n",
    "                if flrg.get_key() not in self.flrgs:\n",
    "                    #NaÃ¯ve approach is applied when no rules were found\n",
    "                    if self.target_variable.name in flrg.LHS:\n",
    "                        fs = flrg.LHS[self.target_variable.name]\n",
    "                        fset = self.target_variable.partitioner.sets[fs]\n",
    "                        up = fset.upper\n",
    "                        lo = fset.lower\n",
    "                        mv = fset.membership(data_point[self.target_variable.name])\n",
    "                        mvs.append(mv)\n",
    "                        ups.append(up)\n",
    "                        los.append(lo)\n",
    "                    else:\n",
    "                        mvs.append(0.)\n",
    "                        ups.append(0.)\n",
    "                        los.append(0.)\n",
    "                else:\n",
    "                    _flrg = self.flrgs[flrg.get_key()]\n",
    "                    mvs.append(_flrg.get_membership(data_point, self.explanatory_variables))\n",
    "                    ups.append(_flrg.get_upper(self.target_variable.partitioner.sets))\n",
    "                    los.append(_flrg.get_lower(self.target_variable.partitioner.sets))\n",
    "\n",
    "            mv = np.array(mvs)\n",
    "            up = np.dot(mv, np.array(ups).T) / np.nansum(mv)\n",
    "            lo = np.dot(mv, np.array(los).T) / np.nansum(mv)\n",
    "\n",
    "            ret.append([lo, up])\n",
    "\n",
    "        ret = self.target_variable.apply_inverse_transformations(ret,\n",
    "                                                           params=data[self.target_variable.data_label].values)\n",
    "        return ret\n",
    "\n",
    "    def forecast_ahead_interval(self, data, steps, **kwargs):\n",
    "        generators = kwargs.get('generators', None)\n",
    "\n",
    "        if generators is None:\n",
    "            raise Exception('You must provide parameter \\'generators\\'! generators is a dict where the keys' +\n",
    "                            ' are the dataframe column names (except the target_variable) and the values are ' +\n",
    "                            'lambda functions that accept one value (the actual value of the variable) '\n",
    "                            ' and return the next value or trained FTS models that accept the actual values and '\n",
    "                            'forecast new ones.')\n",
    "\n",
    "        ndata = self.apply_transformations(data)\n",
    "\n",
    "        start = kwargs.get('start_at', 0)\n",
    "\n",
    "        ret = []\n",
    "        ix = ndata.index[start: start + self.max_lag]\n",
    "        lo = ndata.loc[ix] #[ndata.loc[k] for k in ix]\n",
    "        up = ndata.loc[ix] #[ndata.loc[k] for k in ix]\n",
    "        for k in np.arange(0, steps):\n",
    "            tmp_lo = self.forecast_interval(lo[-self.max_lag:], **kwargs)[0]\n",
    "            tmp_up = self.forecast_interval(up[-self.max_lag:], **kwargs)[0]\n",
    "\n",
    "            ret.append([min(tmp_lo), max(tmp_up)])\n",
    "\n",
    "            new_data_point_lo = {}\n",
    "            new_data_point_up = {}\n",
    "\n",
    "            for data_label in generators.keys():\n",
    "                if data_label != self.target_variable.data_label:\n",
    "                    if isinstance(generators[data_label], LambdaType):\n",
    "                        last_data_point_lo = lo.loc[lo.index[-1]]\n",
    "                        new_data_point_lo[data_label] = generators[data_label](last_data_point_lo[data_label])\n",
    "                        last_data_point_up = up.loc[up.index[-1]]\n",
    "                        new_data_point_up[data_label] = generators[data_label](last_data_point_up[data_label])\n",
    "                    elif isinstance(generators[data_label], fts.FTS):\n",
    "                        model = generators[data_label]\n",
    "                        last_data_point_lo = lo.loc[lo.index[-model.order:]]\n",
    "                        last_data_point_up = up.loc[up.index[-model.order:]]\n",
    "\n",
    "                        if not model.is_multivariate:\n",
    "                            last_data_point_lo = last_data_point_lo[data_label].values\n",
    "                            last_data_point_up = last_data_point_up[data_label].values\n",
    "\n",
    "                        new_data_point_lo[data_label] = model.forecast(last_data_point_lo)[0]\n",
    "                        new_data_point_up[data_label] = model.forecast(last_data_point_up)[0]\n",
    "\n",
    "            new_data_point_lo[self.target_variable.data_label] = min(tmp_lo)\n",
    "            new_data_point_up[self.target_variable.data_label] = max(tmp_up)\n",
    "\n",
    "            lo = lo.append(new_data_point_lo, ignore_index=True)\n",
    "            up = up.append(new_data_point_up, ignore_index=True)\n",
    "\n",
    "        return ret[-steps:]\n",
    "\n",
    "\n",
    "    def clone_parameters(self, model):\n",
    "        super(MVFTS, self).clone_parameters(model)\n",
    "\n",
    "        self.explanatory_variables = model.explanatory_variables\n",
    "        self.target_variable = model.target_variable\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        _str = self.name + \":\\n\"\n",
    "        for k in self.flrgs.keys():\n",
    "            _str += str(self.flrgs[k]) + \"\\n\"\n",
    "\n",
    "        return _str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code for pyFTS.models.multivariate.FLR\n",
    "\n",
    "class FLR(object):\n",
    "    \"\"\"Multivariate Fuzzy Logical Relationship\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a Fuzzy Logical Relationship\n",
    "        :param LHS: Left Hand Side fuzzy set\n",
    "        :param RHS: Right Hand Side fuzzy set\n",
    "        \"\"\"\n",
    "        self.LHS = {}\n",
    "        self.RHS = None\n",
    "\n",
    "    def set_lhs(self, var, set):\n",
    "        self.LHS[var] = set\n",
    "\n",
    "\n",
    "    def set_rhs(self, set):\n",
    "        self.RHS = set\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{} -> {}\".format([k for k in self.LHS.values()], self.RHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code for pyFTS.models.multivariate.flrg\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from pyFTS.common import flrg as flg\n",
    "\n",
    "\n",
    "class FLRG(flg.FLRG):\n",
    "    \"\"\"\n",
    "    Multivariate Fuzzy Logical Rule Group\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FLRG,self).__init__(0,**kwargs)\n",
    "        self.order = kwargs.get('order', 1)\n",
    "        self.LHS = kwargs.get('lhs', {})\n",
    "        self.RHS = set()\n",
    "\n",
    "    def set_lhs(self, var, fset):\n",
    "        if self.order == 1:\n",
    "            self.LHS[var] = fset\n",
    "        else:\n",
    "            if var not in self.LHS:\n",
    "                self.LHS[var] = []\n",
    "            self.LHS[var].append(fset)\n",
    "\n",
    "\n",
    "    def append_rhs(self, fset, **kwargs):\n",
    "        self.RHS.add(fset)\n",
    "\n",
    "\n",
    "    def get_membership(self, data, variables):\n",
    "        mvs = []\n",
    "        for var in variables:\n",
    "            s = self.LHS[var.name]\n",
    "            mvs.append(var.partitioner.sets[s].membership(data[var.name]))\n",
    "\n",
    "        return np.nanmin(mvs)\n",
    "\n",
    "\n",
    "    def get_lower(self, sets):\n",
    "        if self.lower is None:\n",
    "            self.lower = min([sets[rhs].lower for rhs in self.RHS])\n",
    "\n",
    "        return self.lower\n",
    "\n",
    "\n",
    "    def get_upper(self, sets):\n",
    "        if self.upper is None:\n",
    "            self.upper = max([sets[rhs].upper for rhs in self.RHS])\n",
    "\n",
    "        return self.upper\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        _str = \"\"\n",
    "        for k in self.RHS:\n",
    "            _str += \",\" if len(_str) > 0 else \"\"\n",
    "            _str += k\n",
    "\n",
    "        return self.get_key() + \" -> \" + _str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code for pyFTS.models.multivariate.partitioner\n",
    "\n",
    "from pyFTS.partitioners import partitioner\n",
    "from pyFTS.models.multivariate.common import MultivariateFuzzySet, fuzzyfy_instance_clustered\n",
    "from itertools import product\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "class MultivariatePartitioner(partitioner.Partitioner):\n",
    "    \"\"\"\n",
    "    Base class for partitioners which use the MultivariateFuzzySet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultivariatePartitioner, self).__init__(name=\"MultivariatePartitioner\", preprocess=False, **kwargs)\n",
    "\n",
    "        self.type = 'multivariate'\n",
    "        self.sets = {}\n",
    "        self.kdtree = None\n",
    "        self.index = {}\n",
    "        self.explanatory_variables = kwargs.get('explanatory_variables', [])\n",
    "        self.target_variable = kwargs.get('target_variable', None)\n",
    "        self.neighbors = kwargs.get('neighbors', 2)\n",
    "        self.optimize = kwargs.get('optimize', True)\n",
    "        if self.optimize:\n",
    "            self.count = {}\n",
    "        data = kwargs.get('data', None)\n",
    "        self.build(data)\n",
    "        self.uod = {}\n",
    "\n",
    "        self.min = self.target_variable.partitioner.min\n",
    "        self.max = self.target_variable.partitioner.max\n",
    "\n",
    "\n",
    "    def format_data(self, data):\n",
    "        ndata = {}\n",
    "        for var in self.explanatory_variables:\n",
    "            ndata[var.name] = var.partitioner.extractor(data[var.data_label])\n",
    "\n",
    "        return ndata\n",
    "\n",
    "\n",
    "    def build(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def append(self, fset):\n",
    "        self.sets[fset.name] = fset\n",
    "\n",
    "\n",
    "    def prune(self):\n",
    "\n",
    "        if not self.optimize:\n",
    "            return\n",
    "\n",
    "        for fset in [fs for fs in self.sets.keys()]:\n",
    "            if fset not in self.count:\n",
    "                fs = self.sets.pop(fset)\n",
    "                del (fs)\n",
    "\n",
    "        self.build_index()\n",
    "\n",
    "\n",
    "    def search(self, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a search for the nearest fuzzy sets of the point 'data'. This function were designed to work with several\n",
    "        overlapped fuzzy sets.\n",
    "\n",
    "        :param data: the value to search for the nearest fuzzy sets\n",
    "        :param type: the return type: 'index' for the fuzzy set indexes or 'name' for fuzzy set names.\n",
    "        :return: a list with the nearest fuzzy sets\n",
    "        \"\"\"\n",
    "\n",
    "        if self.kdtree is None:\n",
    "            self.build_index()\n",
    "\n",
    "        type = kwargs.get('type', 'index')\n",
    "\n",
    "        ndata = [data[k.name] for k in self.explanatory_variables]\n",
    "        _, ix = self.kdtree.query(ndata, self.neighbors)\n",
    "\n",
    "        if not isinstance(ix, (list, np.ndarray)):\n",
    "            ix = [ix]\n",
    "\n",
    "        if self.optimize:\n",
    "            tmp = []\n",
    "            for k in ix:\n",
    "                tmp.append(self.index[k])\n",
    "                self.count[self.index[k]] = 1\n",
    "\n",
    "        if type == 'name':\n",
    "            return [self.index[k] for k in ix]\n",
    "        elif type == 'index':\n",
    "            return sorted(ix)\n",
    "\n",
    "\n",
    "    def fuzzyfy(self, data, **kwargs):\n",
    "        return fuzzyfy_instance_clustered(data, self, **kwargs)\n",
    "\n",
    "\n",
    "    def change_target_variable(self, variable):\n",
    "        self.target_variable = variable\n",
    "        for fset in self.sets.values():\n",
    "            fset.set_target_variable(variable)\n",
    "        self.min = variable.partitioner.min\n",
    "        self.max = variable.partitioner.max\n",
    "\n",
    "\n",
    "    def build_index(self):\n",
    "\n",
    "        midpoints = []\n",
    "\n",
    "        self.index = {}\n",
    "\n",
    "        for ct, fset in enumerate(self.sets.values()):\n",
    "            mp = []\n",
    "            for vr in self.explanatory_variables:\n",
    "                mp.append(fset.sets[vr.name].centroid)\n",
    "            midpoints.append(mp)\n",
    "            self.index[ct] = fset.name\n",
    "\n",
    "\n",
    "        sys.setrecursionlimit(100000)\n",
    "\n",
    "        self.kdtree = KDTree(midpoints)\n",
    "\n",
    "        sys.setrecursionlimit(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code for pyFTS.models.multivariate.variable\n",
    "\n",
    "import pandas as pd\n",
    "from pyFTS.common import fts, FuzzySet, FLR, Membership, tree\n",
    "from pyFTS.partitioners import Grid\n",
    "from pyFTS.models.multivariate import FLR as MVFLR\n",
    "\n",
    "\n",
    "class Variable:\n",
    "    \"\"\"\n",
    "    A variable of a fuzzy time series multivariate model. Each variable contains its own\n",
    "    transformations and partitioners.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param name:\n",
    "        :param \\**kwargs: See below\n",
    "\n",
    "        :Keyword Arguments:\n",
    "            * *alias* -- Alternative name for the variable\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        \"\"\"A string with the name of the variable\"\"\"\n",
    "        self.alias = kwargs.get('alias', self.name)\n",
    "        \"\"\"A string with the alias of the variable\"\"\"\n",
    "        self.data_label = kwargs.get('data_label', self.name)\n",
    "        \"\"\"A string with the column name on DataFrame\"\"\"\n",
    "        self.type = kwargs.get('type', 'common')\n",
    "        self.data_type = kwargs.get('data_type', None)\n",
    "        \"\"\"The type of the data column on Pandas Dataframe\"\"\"\n",
    "        self.mask = kwargs.get('mask', None)\n",
    "        \"\"\"The mask for format the data column on Pandas Dataframe\"\"\"\n",
    "        self.transformation = kwargs.get('transformation', None)\n",
    "        \"\"\"Pre processing transformation for the variable\"\"\"\n",
    "        self.transformation_params = kwargs.get('transformation_params', None)\n",
    "        self.partitioner = None\n",
    "        \"\"\"UoD partitioner for the variable data\"\"\"\n",
    "        self.alpha_cut = kwargs.get('alpha_cut', 0.0)\n",
    "        \"\"\"Minimal membership value to be considered on fuzzyfication process\"\"\"\n",
    "\n",
    "\n",
    "        if kwargs.get('data', None) is not None:\n",
    "            self.build(**kwargs)\n",
    "\n",
    "    def build(self, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        fs = kwargs.get('partitioner', Grid.GridPartitioner)\n",
    "        mf = kwargs.get('func', Membership.trimf)\n",
    "        np = kwargs.get('npart', 10)\n",
    "        data = kwargs.get('data', None)\n",
    "        kw = kwargs.get('partitioner_specific', {})\n",
    "        self.partitioner = fs(data=data[self.data_label].values, npart=np, func=mf,\n",
    "                              transformation=self.transformation, prefix=self.alias,\n",
    "                              variable=self.name, **kw)\n",
    "\n",
    "        self.partitioner.name = self.name + \" \" + self.partitioner.name\n",
    "\n",
    "\n",
    "    def apply_transformations(self, data, **kwargs):\n",
    "\n",
    "        if kwargs.get('params', None) is not None:\n",
    "            self.transformation_params = kwargs.get('params', None)\n",
    "\n",
    "        if self.transformation is not None:\n",
    "            return self.transformation.apply(data, self.transformation_params)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def apply_inverse_transformations(self, data, **kwargs):\n",
    "\n",
    "        if kwargs.get('params', None) is not None:\n",
    "            self.transformation_params = kwargs.get('params', None)\n",
    "\n",
    "        if self.transformation is not None:\n",
    "            return self.transformation.inverse(data, self.transformation_params)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
